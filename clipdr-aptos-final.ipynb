{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2822650,"sourceType":"datasetVersion","datasetId":1715304}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:28:55.706443Z","iopub.execute_input":"2026-01-07T07:28:55.707222Z","iopub.status.idle":"2026-01-07T07:29:00.945175Z","shell.execute_reply.started":"2026-01-07T07:28:55.707190Z","shell.execute_reply":"2026-01-07T07:29:00.944495Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-wqmglgbb\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-wqmglgbb\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (6.3.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2025.11.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.8.0+cu126)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.23.0+cu126)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.14)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"!git clone https://github.com/xk-huang/OrdinalCLIP.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:29:00.946917Z","iopub.execute_input":"2026-01-07T07:29:00.947180Z","iopub.status.idle":"2026-01-07T07:29:01.093914Z","shell.execute_reply.started":"2026-01-07T07:29:00.947151Z","shell.execute_reply":"2026-01-07T07:29:01.093239Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'OrdinalCLIP' already exists and is not an empty directory.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/working/OrdinalCLIP\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:29:01.095061Z","iopub.execute_input":"2026-01-07T07:29:01.095283Z","iopub.status.idle":"2026-01-07T07:29:01.099090Z","shell.execute_reply.started":"2026-01-07T07:29:01.095257Z","shell.execute_reply":"2026-01-07T07:29:01.098511Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import os\nimport time\nimport copy\nimport math \nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport copy\nimport torch.nn.init as init\nfrom IPython.display import FileLink\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torchvision._internally_replaced_utils import load_state_dict_from_url\nfrom torch import optim\nimport torch.utils.model_zoo as model_zoo\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.optim import lr_scheduler\nfrom clip import clip\nfrom clip.model import CLIP\nimport logging\nfrom scipy.ndimage import gaussian_filter1d\nfrom scipy.signal.windows import triang\nimport torchvision\nfrom torchvision import transforms, datasets, models\nfrom typing import Optional, Callable, List, Type, Union, Any\nfrom ordinalclip.utils import Registry\n\nimport timm\nfrom torchsummary import summary\nfrom tqdm import tqdm\nfrom torchvision.models import densenet121, densenet201\ndevice = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\nprint(f\"Using {device} device\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:29:01.100145Z","iopub.execute_input":"2026-01-07T07:29:01.100661Z","iopub.status.idle":"2026-01-07T07:29:01.116782Z","shell.execute_reply.started":"2026-01-07T07:29:01.100638Z","shell.execute_reply":"2026-01-07T07:29:01.116119Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"train_img_dir = \"/kaggle/input/aptos2019/train_images/train_images\"\nval_img_dir=\"/kaggle/input/aptos2019/val_images/val_images\"\ntest_img_dir = \"/kaggle/input/aptos2019/test_images/test_images\"\n\ntrain_csv_path = \"/kaggle/input/aptos2019/train_1.csv\"\nval_csv_path=\"/kaggle/input/aptos2019/valid.csv\"\ntest_csv_path = \"/kaggle/input/aptos2019/test.csv\"\n\nCLIP_MEAN = [0.48145466, 0.4578275, 0.40821073]\nCLIP_STD  = [0.26862954, 0.26130258, 0.27577711]\n\ntransform = {\n    \"train\": transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomVerticalFlip(),\n        transforms.RandomRotation(degrees=30),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=CLIP_MEAN, std=CLIP_STD),\n    ]),\n    \"test\": transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=CLIP_MEAN, std=CLIP_STD),\n    ])\n}\nclass AptosMapper(Dataset):\n    def __init__(self, csv_file, img_dir, transforms=None):\n        self.labels = pd.read_csv(csv_file)\n        self.img_dir = img_dir\n        self.transform = transforms\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        img_name = self.labels.iloc[idx, 0] + \".png\"\n        label = int(self.labels.iloc[idx, 1])\n        img_path = os.path.join(self.img_dir, img_name)\n        image = Image.open(img_path).convert(\"RGB\")\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\nbatch_size=64\n\ntrain_dataset = AptosMapper(train_csv_path, train_img_dir, transforms=transform[\"train\"])\nval_dataset = AptosMapper(val_csv_path, val_img_dir, transforms=transform[\"test\"])\ntest_dataset = AptosMapper(test_csv_path, test_img_dir, transforms=transform[\"test\"])\n\nlen_trainset = len(train_dataset)\nlen_valset = len(val_dataset)\nlen_test = len(test_dataset)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\nprint(len_trainset,len_valset,len_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:29:01.118689Z","iopub.execute_input":"2026-01-07T07:29:01.118948Z","iopub.status.idle":"2026-01-07T07:29:01.154115Z","shell.execute_reply.started":"2026-01-07T07:29:01.118928Z","shell.execute_reply":"2026-01-07T07:29:01.153628Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"model, preprocess = clip.load(\"RN50\", device=device)\nmodel.eval()\n\nprint(\"CLIP loaded on:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:29:01.154961Z","iopub.execute_input":"2026-01-07T07:29:01.155241Z","iopub.status.idle":"2026-01-07T07:29:04.303222Z","shell.execute_reply.started":"2026-01-07T07:29:01.155209Z","shell.execute_reply":"2026-01-07T07:29:04.302646Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"init_rank_path = None\n\nclass PlainPromptLearner(nn.Module):\n    clip_max_num_tokens = 77\n    rank_tokens_position_candidates = {\"tail\", \"middle\", \"front\"}\n\n    def __init__(\n        self,\n        clip_model: CLIP,\n        num_ranks: int,\n        num_tokens_per_rank: Union[int, List],\n        num_context_tokens: int,\n        rank_tokens_position: str = \"tail\",\n        init_context: Optional[str] = None,\n        rank_specific_context: bool = False,\n    ):\n        super().__init__()\n\n        self.num_ranks = num_ranks\n        self.num_context_tokens = num_context_tokens\n        self.rank_tokens_positon = rank_tokens_position\n\n        dtype = clip_model.token_embedding.weight.dtype\n        context_embeds, _num_context_tokens = self.create_context_embeds(\n            clip_model, num_ranks, num_context_tokens, init_context, rank_specific_context, dtype\n        )\n        num_context_tokens = _num_context_tokens\n        self.context_embeds = nn.Parameter(context_embeds)\n\n        if isinstance(num_tokens_per_rank, int):\n            num_tokens_per_rank = [num_tokens_per_rank] * num_ranks\n        rank_embeds, _num_tokens_per_rank = self.create_rank_embeds(\n            clip_model, num_ranks, num_tokens_per_rank, init_rank_path, dtype, num_context_tokens\n        )\n        num_tokens_per_rank = _num_tokens_per_rank\n        self.rank_embeds = nn.Parameter(rank_embeds)\n        assert len(rank_embeds) == num_ranks\n\n        psudo_sentence_tokens = self.create_psudo_sentence_tokens(\n            num_tokens_per_rank, num_context_tokens, num_ranks\n        )\n        self.register_buffer(\"psudo_sentence_tokens\", psudo_sentence_tokens, persistent=False)\n\n        self.num_context_tokens = num_context_tokens\n        self.num_tokens_per_rank = num_tokens_per_rank\n        if rank_tokens_position not in self.rank_tokens_position_candidates:\n            raise ValueError(f\"Invalid rank_tokens_position: {rank_tokens_position}\")\n        self.rank_tokens_positon = rank_tokens_position\n        self.num_ranks = num_ranks\n        self.embeddings_dim = clip_model.token_embedding.embedding_dim\n\n        self.create_sentence_embeds_template(clip_model, num_ranks, psudo_sentence_tokens)\n\n    def forward(self):\n        context_embeds = self.context_embeds\n\n        if context_embeds.dim() == 2:\n            context_embeds = context_embeds[None].expand(self.num_ranks, *context_embeds.shape)\n\n        sentence_embeds = self.sentence_embeds.clone()\n        if self.rank_tokens_positon == \"tail\":\n            for i in range(self.num_ranks):\n                _num_tokens_per_rank = self.num_tokens_per_rank[i]\n                pure_sentence_length = self.num_context_tokens + _num_tokens_per_rank\n                sentence_embeds[i, 1:1 + pure_sentence_length] = torch.cat(\n                    [context_embeds[i], self.rank_embeds[i, :_num_tokens_per_rank]], dim=0\n                )\n        elif self.rank_tokens_positon == \"front\":\n            for i in range(self.num_ranks):\n                _num_tokens_per_rank = self.num_tokens_per_rank[i]\n                pure_sentence_length = self.num_context_tokens + _num_tokens_per_rank\n                sentence_embeds[i, 1:1 + pure_sentence_length] = torch.cat(\n                    [self.rank_embeds[i, :_num_tokens_per_rank], context_embeds[i]], dim=0\n                )\n        elif self.rank_tokens_positon == \"middle\":\n            for i in range(self.num_ranks):\n                _num_tokens_per_rank = self.num_tokens_per_rank[i]\n                pure_sentence_length = self.num_context_tokens + _num_tokens_per_rank\n                _context_embeds = context_embeds[i]\n                half_range = self.num_context_tokens // 2\n                sentence_embeds[i, 1:1 + pure_sentence_length] = torch.cat(\n                    [\n                        _context_embeds[:half_range],\n                        self.rank_embeds[i, :_num_tokens_per_rank],\n                        _context_embeds[half_range:],\n                    ],\n                    dim=0,\n                )\n        return sentence_embeds\n\n    def create_sentence_embeds_template(self, clip_model, num_ranks, psudo_sentence_tokens):\n        with torch.no_grad():\n            device = clip_model.token_embedding.weight.device\n            dtype = clip_model.token_embedding.weight.dtype\n            \n            null_embed = clip_model.token_embedding(\n                torch.tensor([0], device=device)\n            )[0].to(dtype)\n            \n            sot_embed = clip_model.token_embedding(\n                torch.tensor([49406], device=device)\n            )[0].to(dtype)\n            \n            eot_embed = clip_model.token_embedding(\n                torch.tensor([49407], device=device)\n            )[0].to(dtype)\n            \n            full_stop_embed = clip_model.token_embedding(\n                torch.tensor([269], device=device)\n            )[0].to(dtype)\n\n\n        sentence_embeds = null_embed[None, None].repeat(\n            num_ranks, self.clip_max_num_tokens, 1\n        )\n        argmax_index = psudo_sentence_tokens.argmax(dim=-1)\n        rank_index = torch.arange(num_ranks)\n\n        sentence_embeds[:, 0, :] = sot_embed\n        sentence_embeds[rank_index, argmax_index] = eot_embed\n        sentence_embeds[rank_index, argmax_index - 1] = full_stop_embed\n\n        self.register_buffer(\"sentence_embeds\", sentence_embeds, persistent=False)\n\n    def create_psudo_sentence_tokens(self, num_tokens_per_rank, num_context_tokens, num_ranks):\n        psudo_sentence_tokens = torch.zeros(num_ranks, self.clip_max_num_tokens, dtype=torch.long)\n\n        if isinstance(num_tokens_per_rank, List):\n            assert num_ranks == len(num_tokens_per_rank)\n            for i, _num_tokens_per_rank in enumerate(num_tokens_per_rank):\n                sentence_length = 1 + num_context_tokens + _num_tokens_per_rank + 1 + 1\n                psudo_sentence_tokens[i, :sentence_length] = torch.arange(0, sentence_length, dtype=torch.long)\n        else:\n            sentence_length = 1 + num_context_tokens + num_tokens_per_rank + 1 + 1\n            psudo_sentence_tokens[:, :sentence_length] = torch.arange(0, sentence_length, dtype=torch.long)\n        return psudo_sentence_tokens\n\n    def create_rank_embeds(\n        self, clip_model, num_ranks, num_tokens_per_rank, init_rank_path, dtype, num_context_tokens\n    ):\n        if init_rank_path is not None:\n            rank_names = self.read_rank_file(init_rank_path)\n\n            if len(rank_names) != num_ranks:\n                raise ValueError(\"rank_names length mismatch\")\n\n            _rank_tokens = [clip._tokenizer.encode(rank_name) for rank_name in rank_names]\n            _num_tokens_per_rank = [len(rank_token) for rank_token in _rank_tokens]\n            num_tokens_per_rank = _num_tokens_per_rank\n            max_num_tokens_per_rank = np.max(num_tokens_per_rank)\n\n            rank_tokens = torch.zeros(len(_rank_tokens), max_num_tokens_per_rank, dtype=torch.long)\n            for i, rank_token in enumerate(_rank_tokens):\n                valid_length = self.clip_max_num_tokens - num_context_tokens - 3\n                if len(rank_token) > valid_length:\n                    rank_token = rank_token[:valid_length]\n                    raise ValueError(\"rank tokens too long\")\n                rank_tokens[i, :len(rank_token)] = torch.LongTensor(rank_token)\n\n            rank_embeds = clip_model.token_embedding(rank_tokens).type(dtype)\n            rank_embeds = rank_embeds[:, :max_num_tokens_per_rank]\n        else:\n            embeddings_dim = clip_model.token_embedding.embedding_dim\n            if isinstance(num_tokens_per_rank, List):\n                max_num_tokens_per_rank = np.max(num_tokens_per_rank)\n            else:\n                max_num_tokens_per_rank = num_tokens_per_rank\n            if self.clip_max_num_tokens < num_context_tokens + max_num_tokens_per_rank + 3:\n                raise ValueError(\"rank tokens too long\")\n            rank_embeds = torch.empty((num_ranks, max_num_tokens_per_rank, embeddings_dim), dtype=dtype)\n            nn.init.normal_(rank_embeds, std=0.02)\n\n        return rank_embeds, num_tokens_per_rank\n\n    def read_rank_file(self, init_rank_path):\n        rank_names = []\n        with open(init_rank_path, \"r\") as f:\n            for line in f.readlines():\n                line = line.strip().replace(\"_\", \" \")\n                rank_names.append(line)\n        return rank_names\n\n    def create_context_embeds(\n        self,\n        clip_model,\n        num_ranks: int,\n        num_context_tokens: int,\n        init_context: Optional[str],\n        rank_specific_context: bool,\n        dtype,\n    ):\n        if init_context is not None:\n            init_context = init_context.replace(\"_\", \" \")\n            prompt_tokens = clip.tokenize(init_context)[0]\n            _num_context_tokens = torch.argmax(prompt_tokens).item() - 1\n            num_context_tokens = _num_context_tokens\n\n            with torch.no_grad():\n                context_embeds = clip_model.token_embedding(prompt_tokens).type(dtype)\n            context_embeds = context_embeds[1:1 + num_context_tokens]\n\n            if rank_specific_context:\n                context_embeds = context_embeds[None].repeat(num_ranks, 1, 1)\n        else:\n            embeds_dim = clip_model.token_embedding.embedding_dim\n            if rank_specific_context:\n                context_embeds = torch.empty((num_ranks, num_context_tokens, embeds_dim), dtype=dtype)\n            else:\n                context_embeds = torch.empty((num_context_tokens, embeds_dim), dtype=dtype)\n            nn.init.normal_(context_embeds, std=0.02)\n\n        return context_embeds, num_context_tokens\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:29:04.304108Z","iopub.execute_input":"2026-01-07T07:29:04.304383Z","iopub.status.idle":"2026-01-07T07:29:04.327004Z","shell.execute_reply.started":"2026-01-07T07:29:04.304334Z","shell.execute_reply":"2026-01-07T07:29:04.326416Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"def load_clip(\n    text_encoder_name: str,\n    image_encoder_name: str,\n    device: str\n):\n    clip_model, _ = clip.load(text_encoder_name, device=device)\n    clip_model = clip_model.float()\n\n    if image_encoder_name != text_encoder_name:\n        embed_dim = clip_model.text_projection.shape[1]\n        input_resolution = clip_model.visual.input_resolution\n\n        MODEL = getattr(models, image_encoder_name, None)\n        if MODEL is None:\n            raise ValueError(f\"Invalid torchvision model: {image_encoder_name}\")\n\n        clip_model.visual = MODEL(num_classes=embed_dim)\n        clip_model.visual.input_resolution = input_resolution\n\n    return clip_model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:29:04.327843Z","iopub.execute_input":"2026-01-07T07:29:04.328092Z","iopub.status.idle":"2026-01-07T07:29:04.347306Z","shell.execute_reply.started":"2026-01-07T07:29:04.328061Z","shell.execute_reply":"2026-01-07T07:29:04.346759Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"class TextEncoder(nn.Module):\n    def __init__(self, clip_model):\n        super().__init__()\n        self.transformer = clip_model.transformer\n        self.positional_embedding = clip_model.positional_embedding\n        self.ln_final = clip_model.ln_final\n        self.text_projection = clip_model.text_projection\n\n    def forward(self, prompts, tokenized_prompts):\n        x = prompts + self.positional_embedding\n        x = x.permute(1, 0, 2)\n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)\n        x = self.ln_final(x)\n\n        x = x[\n            torch.arange(x.shape[0]),\n            tokenized_prompts.argmax(dim=-1)\n        ] @ self.text_projection\n\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:29:04.348063Z","iopub.execute_input":"2026-01-07T07:29:04.348313Z","iopub.status.idle":"2026-01-07T07:29:04.363985Z","shell.execute_reply.started":"2026-01-07T07:29:04.348287Z","shell.execute_reply":"2026-01-07T07:29:04.363321Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def forward_text_only(self):\n        sentence_embeds = self.prompt_learner()\n        psudo_sentence_tokens = self.psudo_sentence_tokens\n        text_features = self.text_encoder(sentence_embeds, psudo_sentence_tokens)\n\n        return text_features\n\ndef encode_image(self, x):\n        return self.image_encoder(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:29:04.364765Z","iopub.execute_input":"2026-01-07T07:29:04.365011Z","iopub.status.idle":"2026-01-07T07:29:04.377044Z","shell.execute_reply.started":"2026-01-07T07:29:04.364991Z","shell.execute_reply":"2026-01-07T07:29:04.376518Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"class CLIPDR(nn.Module):\n    def __init__(self, clip_model, prompt_learner):\n        super().__init__()\n        self.image_encoder = clip_model.visual\n        self.transformer = clip_model.transformer\n        self.positional_embedding = clip_model.positional_embedding\n        self.ln_final = clip_model.ln_final\n        self.text_projection = clip_model.text_projection\n        self.logit_scale = clip_model.logit_scale\n        self.prompt_learner = prompt_learner\n        self.psudo_sentence_tokens = prompt_learner.psudo_sentence_tokens\n        self.embed_dims = clip_model.text_projection.shape[1]\n        self.num_ranks = self.prompt_learner.num_ranks\n        self.image_encoder = clip_model.visual\n        self.text_encoder = TextEncoder(clip_model)\n    \n    \n    \n    def forward(self, images):\n        sentence_embeds = self.prompt_learner()\n        psudo_sentence_tokens = self.psudo_sentence_tokens\n        text_features = self.text_encoder(sentence_embeds, psudo_sentence_tokens)\n        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n        image_features = self.image_encoder(images)\n        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n        \n        # Compute logits\n        logit_scale = self.logit_scale.exp()\n        logits = logit_scale * image_features @ text_features.t()\n        \n        # Return raw features too for FDS\n        return logits, image_features, text_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:29:04.377999Z","iopub.execute_input":"2026-01-07T07:29:04.378272Z","iopub.status.idle":"2026-01-07T07:29:04.392205Z","shell.execute_reply.started":"2026-01-07T07:29:04.378242Z","shell.execute_reply":"2026-01-07T07:29:04.391636Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n\nclip_model, _ = clip.load(\"RN50\", device=device)\nclip_model = clip_model.float()\nprint(\"CLIP model loaded\")\n\nprompt_learner = PlainPromptLearner(\n    clip_model=clip_model,\n    num_ranks=5,\n    num_tokens_per_rank=1,\n    num_context_tokens=10,\n    rank_tokens_position=\"tail\",\n    init_context=None,\n    rank_specific_context=False\n).to(device)\nprint(\"Prompt learner created\")\n\nmodel = CLIPDR(\n    clip_model=clip_model,\n    prompt_learner=prompt_learner\n).to(device)\nprint(\"CLIPDR model created\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:29:04.392979Z","iopub.execute_input":"2026-01-07T07:29:04.393275Z","iopub.status.idle":"2026-01-07T07:29:07.449736Z","shell.execute_reply.started":"2026-01-07T07:29:04.393249Z","shell.execute_reply":"2026-01-07T07:29:07.448989Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"prompt_learner = PlainPromptLearner(\n    clip_model=clip_model,\n    num_ranks=5,\n    num_tokens_per_rank=1,\n    num_context_tokens=10,\n    rank_tokens_position=\"tail\",\n    init_context=None,\n    rank_specific_context=False\n).to(device)\n\nmodel = CLIPDR(\n    clip_model=clip_model,\n    prompt_learner=prompt_learner\n).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:29:07.450522Z","iopub.execute_input":"2026-01-07T07:29:07.450715Z","iopub.status.idle":"2026-01-07T07:29:07.460665Z","shell.execute_reply.started":"2026-01-07T07:29:07.450696Z","shell.execute_reply":"2026-01-07T07:29:07.459882Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def calibrate_mean_var(matrix, m1, v1, m2, v2, clip_min=0.5, clip_max=2.0):\n    if torch.sum(v1) < 1e-10:\n        return matrix\n\n    if (v1 <= 0.).any() or (v2 < 0.).any():\n        valid_pos = (((v1 > 0.) + (v2 >= 0.)) == 2)\n        factor = torch.clamp(v2[valid_pos] / v1[valid_pos], clip_min, clip_max)\n        matrix[:, valid_pos] = (\n            matrix[:, valid_pos] - m1[valid_pos]\n        ) * torch.sqrt(factor) + m2[valid_pos]\n        return matrix\n\n    factor = torch.clamp(v2 / v1, clip_min, clip_max)\n    return (matrix - m1) * torch.sqrt(factor) + m2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:29:07.463025Z","iopub.execute_input":"2026-01-07T07:29:07.463347Z","iopub.status.idle":"2026-01-07T07:29:07.468521Z","shell.execute_reply.started":"2026-01-07T07:29:07.463324Z","shell.execute_reply":"2026-01-07T07:29:07.467748Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"print = logging.info\n\n\nclass FDS(nn.Module):\n\n    def __init__(self, feature_dim, bucket_num=100, bucket_start=3, start_update=0, start_smooth=1,\n                 kernel='gaussian', ks=5, sigma=2, momentum=0.9):\n        super(FDS, self).__init__()\n        self.feature_dim = feature_dim\n        self.bucket_num = bucket_num\n        self.bucket_start = bucket_start\n        self.kernel_window = self._get_kernel_window(kernel, ks, sigma)\n        self.half_ks = (ks - 1) // 2\n        self.momentum = momentum\n        self.start_update = start_update\n        self.start_smooth = start_smooth\n\n        self.register_buffer('epoch', torch.zeros(1).fill_(start_update))\n        self.register_buffer('running_mean', torch.zeros(bucket_num - bucket_start, feature_dim))\n        self.register_buffer('running_var', torch.ones(bucket_num - bucket_start, feature_dim))\n        self.register_buffer('running_mean_last_epoch', torch.zeros(bucket_num - bucket_start, feature_dim))\n        self.register_buffer('running_var_last_epoch', torch.ones(bucket_num - bucket_start, feature_dim))\n        self.register_buffer('smoothed_mean_last_epoch', torch.zeros(bucket_num - bucket_start, feature_dim))\n        self.register_buffer('smoothed_var_last_epoch', torch.ones(bucket_num - bucket_start, feature_dim))\n        self.register_buffer('num_samples_tracked', torch.zeros(bucket_num - bucket_start))\n\n    @staticmethod\n    def _get_kernel_window(kernel, ks, sigma):\n        assert kernel in ['gaussian', 'triang', 'laplace']\n        half_ks = (ks - 1) // 2\n        if kernel == 'gaussian':\n            base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks\n            base_kernel = np.array(base_kernel, dtype=np.float32)\n            kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / sum(gaussian_filter1d(base_kernel, sigma=sigma))\n        elif kernel == 'triang':\n            kernel_window = triang(ks) / sum(triang(ks))\n        else:\n            laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)\n            kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / sum(map(laplace, np.arange(-half_ks, half_ks + 1)))\n\n        print(f'Using FDS: [{kernel.upper()}] ({ks}/{sigma})')\n        return torch.tensor(kernel_window, dtype=torch.float32).cuda()\n\n    def _update_last_epoch_stats(self):\n        self.running_mean_last_epoch = self.running_mean\n        self.running_var_last_epoch = self.running_var\n\n        self.smoothed_mean_last_epoch = F.conv1d(\n            input=F.pad(self.running_mean_last_epoch.unsqueeze(1).permute(2, 1, 0),\n                        pad=(self.half_ks, self.half_ks), mode='reflect'),\n            weight=self.kernel_window.view(1, 1, -1), padding=0\n        ).permute(2, 1, 0).squeeze(1)\n        self.smoothed_var_last_epoch = F.conv1d(\n            input=F.pad(self.running_var_last_epoch.unsqueeze(1).permute(2, 1, 0),\n                        pad=(self.half_ks, self.half_ks), mode='reflect'),\n            weight=self.kernel_window.view(1, 1, -1), padding=0\n        ).permute(2, 1, 0).squeeze(1)\n\n    def reset(self):\n        self.running_mean.zero_()\n        self.running_var.fill_(1)\n        self.running_mean_last_epoch.zero_()\n        self.running_var_last_epoch.fill_(1)\n        self.smoothed_mean_last_epoch.zero_()\n        self.smoothed_var_last_epoch.fill_(1)\n        self.num_samples_tracked.zero_()\n\n    def update_last_epoch_stats(self, epoch):\n        if epoch == self.epoch + 1:\n            self.epoch += 1\n            self._update_last_epoch_stats()\n            print(f\"Updated smoothed statistics on Epoch [{epoch}]!\")\n\n    def update_running_stats(self, features, labels, epoch):\n        #if epoch < self.epoch:\n        #    return\n\n        assert self.feature_dim == features.size(1), \"Input feature dimension is not aligned!\"\n        assert features.size(0) == labels.size(0), \"Dimensions of features and labels are not aligned!\"\n\n        for label in torch.unique(labels):\n            if label > self.bucket_num - 1 or label < self.bucket_start:\n                continue\n            elif label == self.bucket_start:\n                curr_feats = features[labels <= label]\n            elif label == self.bucket_num - 1:\n                curr_feats = features[labels >= label]\n            else:\n                curr_feats = features[labels == label]\n            curr_num_sample = curr_feats.size(0)\n            curr_mean = torch.mean(curr_feats, 0)\n            curr_var = torch.var(curr_feats, 0, unbiased=True if curr_feats.size(0) != 1 else False)\n\n            self.num_samples_tracked[int(label - self.bucket_start)] += curr_num_sample\n            factor = self.momentum if self.momentum is not None else \\\n                (1 - curr_num_sample / float(self.num_samples_tracked[int(label - self.bucket_start)]))\n            factor = 0 if epoch == self.start_update else factor\n            self.running_mean[int(label - self.bucket_start)] = \\\n                (1 - factor) * curr_mean + factor * self.running_mean[int(label - self.bucket_start)]\n            self.running_var[int(label - self.bucket_start)] = \\\n                (1 - factor) * curr_var + factor * self.running_var[int(label - self.bucket_start)]\n\n        print(f\"Updated running statistics with Epoch [{epoch}] features!\")\n\n    def smooth(self, features, labels, epoch):\n        #if epoch < self.start_smooth:\n            #return features\n\n        #labels = labels.squeeze(1)\n        for label in torch.unique(labels):\n            if label > self.bucket_num - 1 or label < self.bucket_start:\n                continue\n            elif label == self.bucket_start:\n                features[labels <= label] = calibrate_mean_var(\n                    features[labels <= label],\n                    self.running_mean_last_epoch[int(label - self.bucket_start)],\n                    self.running_var_last_epoch[int(label - self.bucket_start)],\n                    self.smoothed_mean_last_epoch[int(label - self.bucket_start)],\n                    self.smoothed_var_last_epoch[int(label - self.bucket_start)])\n            elif label == self.bucket_num - 1:\n                features[labels >= label] = calibrate_mean_var(\n                    features[labels >= label],\n                    self.running_mean_last_epoch[int(label - self.bucket_start)],\n                    self.running_var_last_epoch[int(label - self.bucket_start)],\n                    self.smoothed_mean_last_epoch[int(label - self.bucket_start)],\n                    self.smoothed_var_last_epoch[int(label - self.bucket_start)])\n            else:\n                features[labels == label] = calibrate_mean_var(\n                    features[labels == label],\n                    self.running_mean_last_epoch[int(label - self.bucket_start)],\n                    self.running_var_last_epoch[int(label - self.bucket_start)],\n                    self.smoothed_mean_last_epoch[int(label - self.bucket_start)],\n                    self.smoothed_var_last_epoch[int(label - self.bucket_start)])\n        return features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:29:07.469283Z","iopub.execute_input":"2026-01-07T07:29:07.469498Z","iopub.status.idle":"2026-01-07T07:29:07.748153Z","shell.execute_reply.started":"2026-01-07T07:29:07.469476Z","shell.execute_reply":"2026-01-07T07:29:07.747436Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"import math\n\nimport torch\nfrom torch.optim.optimizer import Optimizer, required\n\n\nclass RAdam(Optimizer):\n    def __init__(\n        self,\n        params,\n        lr=1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n        degenerated_to_sgd=False,\n    ):\n        if not 0.0 <= lr:\n            raise ValueError(f\"Invalid learning rate: {lr}\")\n        if not 0.0 <= eps:\n            raise ValueError(f\"Invalid epsilon value: {eps}\")\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n\n        self.degenerated_to_sgd = degenerated_to_sgd\n        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n            for param in params:\n                if \"betas\" in param and (param[\"betas\"][0] != betas[0] or param[\"betas\"][1] != betas[1]):\n                    param[\"buffer\"] = [[None, None, None] for _ in range(10)]\n        defaults = dict(\n            lr=lr,\n            betas=betas,\n            eps=eps,\n            weight_decay=weight_decay,\n            buffer=[[None, None, None] for _ in range(10)],\n        )\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\"RAdam does not support sparse gradients\")\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[\"exp_avg\"] = state[\"exp_avg\"].type_as(p_data_fp32)\n                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n                beta1, beta2 = group[\"betas\"]\n\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n\n                state[\"step\"] += 1\n                buffered = group[\"buffer\"][int(state[\"step\"] % 10)]\n                if state[\"step\"] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[\"step\"]\n                    beta2_t = beta2 ** state[\"step\"]\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[\"step\"] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = math.sqrt(\n                            (1 - beta2_t)\n                            * (N_sma - 4)\n                            / (N_sma_max - 4)\n                            * (N_sma - 2)\n                            / N_sma\n                            * N_sma_max\n                            / (N_sma_max - 2)\n                        ) / (1 - beta1 ** state[\"step\"])\n                    elif self.degenerated_to_sgd:\n                        step_size = 1.0 / (1 - beta1 ** state[\"step\"])\n                    else:\n                        step_size = -1\n                    buffered[2] = step_size\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:\n                    if group[\"weight_decay\"] != 0:\n                        p_data_fp32.add_(p_data_fp32, alpha=-group[\"weight_decay\"] * group[\"lr\"])\n                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n                    p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size * group[\"lr\"])\n                    p.data.copy_(p_data_fp32)\n                elif step_size > 0:\n                    if group[\"weight_decay\"] != 0:\n                        p_data_fp32.add_(p_data_fp32, alpha=-group[\"weight_decay\"] * group[\"lr\"])\n                    p_data_fp32.add_(exp_avg, alpha=-step_size * group[\"lr\"])\n                    p.data.copy_(p_data_fp32)\n\n        return loss\n\n\nclass PlainRAdam(Optimizer):\n    def __init__(\n        self,\n        params,\n        lr=1e-3,\n        betas=(0.9, 0.999),\n        eps=1e-8,\n        weight_decay=0,\n        degenerated_to_sgd=False,\n    ):\n        if not 0.0 <= lr:\n            raise ValueError(f\"Invalid learning rate: {lr}\")\n        if not 0.0 <= eps:\n            raise ValueError(f\"Invalid epsilon value: {eps}\")\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n\n        self.degenerated_to_sgd = degenerated_to_sgd\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n\n        super(PlainRAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(PlainRAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\"RAdam does not support sparse gradients\")\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[\"exp_avg\"] = state[\"exp_avg\"].type_as(p_data_fp32)\n                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n                beta1, beta2 = group[\"betas\"]\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state[\"step\"] += 1\n                beta2_t = beta2 ** state[\"step\"]\n                N_sma_max = 2 / (1 - beta2) - 1\n                N_sma = N_sma_max - 2 * state[\"step\"] * beta2_t / (1 - beta2_t)\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:\n                    if group[\"weight_decay\"] != 0:\n                        p_data_fp32.add_(-group[\"weight_decay\"] * group[\"lr\"], p_data_fp32)\n                    step_size = (\n                        group[\"lr\"]\n                        * math.sqrt(\n                            (1 - beta2_t)\n                            * (N_sma - 4)\n                            / (N_sma_max - 4)\n                            * (N_sma - 2)\n                            / N_sma\n                            * N_sma_max\n                            / (N_sma_max - 2)\n                        )\n                        / (1 - beta1 ** state[\"step\"])\n                    )\n                    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n                    p.data.copy_(p_data_fp32)\n                elif self.degenerated_to_sgd:\n                    if group[\"weight_decay\"] != 0:\n                        p_data_fp32.add_(-group[\"weight_decay\"] * group[\"lr\"], p_data_fp32)\n                    step_size = group[\"lr\"] / (1 - beta1 ** state[\"step\"])\n                    p_data_fp32.add_(-step_size, exp_avg)\n                    p.data.copy_(p_data_fp32)\n\n        return loss\n\n\nclass AdamW(Optimizer):\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, warmup=0):\n        if not 0.0 <= lr:\n            raise ValueError(f\"Invalid learning rate: {lr}\")\n        if not 0.0 <= eps:\n            raise ValueError(f\"Invalid epsilon value: {eps}\")\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, warmup=warmup)\n        super(AdamW, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(AdamW, self).__setstate__(state)\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[\"step\"] = 0\n                    state[\"exp_avg\"] = torch.zeros_like(p_data_fp32)\n                    state[\"exp_avg_sq\"] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[\"exp_avg\"] = state[\"exp_avg\"].type_as(p_data_fp32)\n                    state[\"exp_avg_sq\"] = state[\"exp_avg_sq\"].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n                beta1, beta2 = group[\"betas\"]\n\n                state[\"step\"] += 1\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n                bias_correction1 = 1 - beta1 ** state[\"step\"]\n                bias_correction2 = 1 - beta2 ** state[\"step\"]\n\n                if group[\"warmup\"] > state[\"step\"]:\n                    scheduled_lr = 1e-8 + state[\"step\"] * group[\"lr\"] / group[\"warmup\"]\n                else:\n                    scheduled_lr = group[\"lr\"]\n\n                step_size = scheduled_lr * math.sqrt(bias_correction2) / bias_correction1\n\n                if group[\"weight_decay\"] != 0:\n                    p_data_fp32.add_(-group[\"weight_decay\"] * scheduled_lr, p_data_fp32)\n\n                p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:29:07.749386Z","iopub.execute_input":"2026-01-07T07:29:07.749708Z","iopub.status.idle":"2026-01-07T07:29:07.779896Z","shell.execute_reply.started":"2026-01-07T07:29:07.749682Z","shell.execute_reply":"2026-01-07T07:29:07.779213Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"optimizer = RAdam(\n    model.parameters(),\n    lr=0.0001,\n    betas=(0.9, 0.999),\n    weight_decay=0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:29:07.780791Z","iopub.execute_input":"2026-01-07T07:29:07.781555Z","iopub.status.idle":"2026-01-07T07:29:07.796709Z","shell.execute_reply.started":"2026-01-07T07:29:07.781512Z","shell.execute_reply":"2026-01-07T07:29:07.796016Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"scheduler = torch.optim.lr_scheduler.MultiStepLR(\n    optimizer,\n    milestones=[60],\n    gamma=0.1\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:29:07.797608Z","iopub.execute_input":"2026-01-07T07:29:07.797844Z","iopub.status.idle":"2026-01-07T07:29:07.810125Z","shell.execute_reply.started":"2026-01-07T07:29:07.797814Z","shell.execute_reply":"2026-01-07T07:29:07.809477Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom sklearn.metrics import roc_auc_score, accuracy_score, f1_score, roc_curve, auc\n\n# REQUIRED imports\n# make sure these exist in your project\n# from optim import RAdam\n# from fds import FDS\n\n\nclass Runner(pl.LightningModule):\n    def __init__(self, model, num_ranks=5):\n        super().__init__()\n        self.model = model\n        self.num_ranks = num_ranks\n        \n        self.ce_loss = nn.CrossEntropyLoss()\n        self.kl_loss = nn.KLDivLoss(reduction=\"sum\")\n        \n        self.FDS = FDS(\n            feature_dim=5,\n            bucket_num=100,\n            bucket_start=3,\n            start_update=0,\n            start_smooth=1,\n            kernel='gaussian',\n            ks=5,\n            sigma=2,\n            momentum=0.9\n        )\n        \n        self.register_buffer(\n            \"rank_output_value_array\",\n            torch.arange(0, num_ranks).float(),\n            persistent=False\n        )\n    \n    def forward(self, x):\n        return self.model(x)\n\n    def forward_text_only(self):\n        return self.forward_text_only()\n    \n    def run_step(self, batch, batch_idx, M):\n        x, y = batch\n        \n        our_logits, image_features, text_features = self.model(x)\n        our_logits = our_logits.float()\n        \n        if M == 0:\n            rank_loss = self.rank_loss(our_logits, y)\n            loss_kl = self.compute_kl_loss(our_logits, y)\n            loss_ce = self.ce_loss(our_logits, y)\n            loss = loss_ce + loss_kl + rank_loss\n        else:\n            loss_kl = self.compute_kl_loss(our_logits, y)\n            loss_ce = self.ce_loss(our_logits, y)\n            loss = loss_ce + loss_kl\n        \n        metrics_exp = self.compute_per_example_metrics(our_logits, y, \"exp\")\n        metrics_max = self.compute_per_example_metrics(our_logits, y, \"max\")\n        \n        return {\"loss\": loss, **metrics_exp, **metrics_max}\n    \n    def training_step(self, batch, batch_idx):\n        outputs = self.run_step(batch, batch_idx, M=0)\n        self.logging(outputs, \"train\", on_step=True, on_epoch=True)\n        return outputs\n    \n    def validation_step(self, batch, batch_idx):\n        outputs = self.run_step(batch, batch_idx, M=1)\n        self.logging(outputs, \"val\", on_step=False, on_epoch=True)\n        return outputs\n    \n    def test_step(self, batch, batch_idx):\n        outputs = self.run_step(batch, batch_idx, M=0)\n        self.logging(outputs, \"test\", on_step=False, on_epoch=True)\n        return outputs\n    \n    def compute_kl_loss(self, logits, y):\n        y_t = F.one_hot(y, self.num_ranks).t()\n        y_t_row_ind = y_t.sum(-1) > 0\n        num_slots = y_t_row_ind.sum()\n        y_t_reduction = (y_t * 10.0).softmax(-1)\n        y_t_reduction[y_t_row_ind <= 0] = 0\n        logits_t = logits.t()\n        return self.kl_loss(F.log_softmax(logits_t, dim=-1), y_t_reduction) / num_slots\n    \n    def rank_loss(self, our_logits, y):\n        indexA = torch.nonzero(y == 0, as_tuple=True)[0]\n        indexB = torch.nonzero(y == 1, as_tuple=True)[0]\n        indexC = torch.nonzero(y == 2, as_tuple=True)[0]\n        indexD = torch.nonzero(y == 3, as_tuple=True)[0]\n        indexF = torch.nonzero(y == 4, as_tuple=True)[0]\n        \n        images_similarity1 = torch.zeros(len(y), 5, device=our_logits.device)\n        images_similarity2 = torch.zeros(len(y), 5, device=our_logits.device)\n        images_similarity3 = torch.zeros(len(y), 5, device=our_logits.device)\n        \n        logits_similarity_image1 = torch.zeros_like(images_similarity1)\n        logits_similarity_image2 = torch.zeros_like(images_similarity2)\n        logits_similarity_image3 = torch.zeros_like(images_similarity3)\n        \n        for index in indexA:\n            images_similarity1[index, 0] = 1\n            logits_similarity_image1[index, :2] = our_logits[index, :2]\n            logits_similarity_image2[index, 1:3] = our_logits[index, 1:3]\n            logits_similarity_image3[index, 2:4] = our_logits[index, 2:4]\n        \n        for index in indexB:\n            images_similarity1[index, 1] = 1\n            logits_similarity_image1[index, 1:3] = our_logits[index, 1:3]\n            logits_similarity_image2[index, 2:4] = our_logits[index, 2:4]\n            logits_similarity_image3[index, 3:5] = our_logits[index, 3:5]\n        \n        for index in indexC:\n            images_similarity1[index, 2] = 1\n            logits_similarity_image1[index, 2:4] = our_logits[index, 2:4]\n            logits_similarity_image2[index, 3:5] = our_logits[index, 3:5]\n        \n        for index in indexD:\n            images_similarity1[index, 3] = 1\n            logits_similarity_image1[index, 3:5] = our_logits[index, 3:5]\n        \n        for index in indexF:\n            images_similarity1[index, 4] = 1\n            logits_similarity_image1[index, 4] = our_logits[index, 4]\n        \n        loss1 = nn.CrossEntropyLoss()(logits_similarity_image1, images_similarity1)\n        loss2 = nn.CrossEntropyLoss()(logits_similarity_image2, images_similarity2)\n        loss3 = nn.CrossEntropyLoss()(logits_similarity_image3, images_similarity3)\n        \n        return loss1 + loss2 + loss3\n    \n    def compute_per_example_metrics(self, logits, y, gather_type=\"exp\"):\n        probs = F.softmax(logits, -1)\n        dtype = logits.dtype\n        \n        if gather_type == \"exp\":\n            predict_y = torch.sum(\n                probs * self.rank_output_value_array.type(dtype),\n                dim=-1\n            )\n        else:\n            predict_y = torch.argmax(probs, dim=-1).type(dtype)\n        \n        mae = torch.abs(predict_y - y)\n        acc = (torch.round(predict_y) == y).type(dtype)\n        \n        auc_ovo = roc_auc_score(\n            y.cpu().numpy(),\n            probs.detach().cpu().numpy(),\n            average='macro',\n            multi_class='ovo',\n            labels=[0, 1, 2, 3, 4]\n        )\n        auc_ovo = torch.tensor(auc_ovo)\n        \n        f1 = f1_score(\n            y.cpu().numpy(),\n            torch.round(predict_y).detach().cpu().numpy(),  # FIXED: Added .detach()\n            average='macro'\n        )\n        f1 = torch.tensor(f1)\n        \n        return {\n            f\"mae_{gather_type}_metric\": mae,\n            f\"acc_{gather_type}_metric\": acc,\n            f\"{gather_type}_DGDR_auc_metric\": auc_ovo,\n            f\"{gather_type}_DGDR_f1_metric\": f1\n        }\n    \n    def logging(self, outputs, run_type, on_step=True, on_epoch=True):\n        for k, v in outputs.items():\n            if k.endswith(\"metric\") or k.endswith(\"loss\"):\n                self.log(\n                    f\"{run_type}_{k}\",\n                    v.mean(),\n                    on_step=on_step,\n                    on_epoch=on_epoch,\n                    prog_bar=True,\n                    logger=True\n                )\n    \n    def configure_optimizers(self):\n        params = [\n            {\"params\": self.model.prompt_learner.context_embeds, \"lr\": 1e-4},\n            {\"params\": self.model.prompt_learner.rank_embeds, \"lr\": 1e-4},\n            {\"params\": self.model.image_encoder.parameters(), \"lr\": 1e-4},\n        ]\n        \n        optimizer = RAdam(\n            params,\n            lr=1e-4,\n            betas=(0.9, 0.999),\n            weight_decay=0,\n            degenerated_to_sgd=False\n        )\n        \n        scheduler = torch.optim.lr_scheduler.MultiStepLR(\n            optimizer,\n            milestones=[60],\n            gamma=0.1\n        )\n        \n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"interval\": \"epoch\",\n                \"frequency\": 1\n            }\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:38:19.286523Z","iopub.execute_input":"2026-01-07T07:38:19.286892Z","iopub.status.idle":"2026-01-07T07:38:19.310699Z","shell.execute_reply.started":"2026-01-07T07:38:19.286850Z","shell.execute_reply":"2026-01-07T07:38:19.310044Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# ============= TRAINING CODE =============\n# Create checkpoint callback\ncheckpoint_callback = ModelCheckpoint(\n    monitor='val_acc_exp_metric',  # Metric to monitor\n    dirpath='checkpoints/',         # Where to save\n    filename='best-model-{epoch:02d}-{val_acc_exp_metric:.2f}',\n    save_top_k=1,                   # Save only the best model\n    mode='max',                     # 'max' for accuracy, 'min' for loss\n    save_last=True                  # Also save the last checkpoint\n)\n\n# Create runner and trainer\nrunner = Runner(model)\ntrainer = pl.Trainer(\n    max_epochs=100, \n    accelerator='gpu', \n    devices=1,\n    callbacks=[checkpoint_callback]\n)\n\n# Train the model\ntrainer.fit(runner, train_loader, val_loader)\n\nprint(\"Training completed! Best model saved in checkpoints/ directory\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-07T07:38:21.059932Z","iopub.execute_input":"2026-01-07T07:38:21.060564Z"}},"outputs":[{"name":"stderr","text":"INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\nINFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\nINFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\n\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mName   \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mType            \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m\n\n\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m model    CLIPDR            76.7 M  train      0 \n\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m ce_loss  CrossEntropyLoss       0  train      0 \n\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m kl_loss  KLDivLoss              0  train      0 \n\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m FDS      FDS                    0  train      0 \n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name    </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type             </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>\n\n<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span> model    CLIPDR            76.7 M  train      0 \n<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span> ce_loss  CrossEntropyLoss       0  train      0 \n<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span> kl_loss  KLDivLoss              0  train      0 \n<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span> FDS      FDS                    0  train      0 \n\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1mTrainable params\u001b[0m: 76.7 M                                                                                           \n\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n\u001b[1mTotal params\u001b[0m: 76.7 M                                                                                               \n\u001b[1mTotal estimated model params size (MB)\u001b[0m: 306                                                                        \n\u001b[1mModules in train mode\u001b[0m: 6                                                                                           \n\u001b[1mModules in eval mode\u001b[0m: 323                                                                                          \n\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 76.7 M                                                                                           \n<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n<span style=\"font-weight: bold\">Total params</span>: 76.7 M                                                                                               \n<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 306                                                                        \n<span style=\"font-weight: bold\">Modules in train mode</span>: 6                                                                                           \n<span style=\"font-weight: bold\">Modules in eval mode</span>: 323                                                                                          \n<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9a7dc41d2b643c48d8b213a38110d79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/usr/local/lib/python3.12/dist-packages/pytorch_lightning/loops/fit_loop.py:317: The number of training batches \n(46) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if\nyou want to see logs for the training epoch.\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.12/dist-packages/pytorch_lightning/loops/fit_loop.py:317: The number of training batches \n(46) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if\nyou want to see logs for the training epoch.\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/usr/local/lib/python3.12/dist-packages/pytorch_lightning/loops/fit_loop.py:534: Found 323 module(s) in eval mode \nat the start of training. This may lead to unexpected behavior during training. If this is intentional, you can \nignore this warning.\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.12/dist-packages/pytorch_lightning/loops/fit_loop.py:534: Found 323 module(s) in eval mode \nat the start of training. This may lead to unexpected behavior during training. If this is intentional, you can \nignore this warning.\n</pre>\n"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# ============= TESTING CODE =============\n\n# Find the best checkpoint\nimport glob\ncheckpoint_files = glob.glob('checkpoints/best-model-*.ckpt')\nbest_ckpt = checkpoint_files[0] if checkpoint_files else 'checkpoints/last.ckpt'\n\nprint(f\"\\nLoading best model from: {best_ckpt}\")\n\n# Create new runner for testing\ntest_runner = Runner(model)\n\n# Create test trainer\ntest_trainer = pl.Trainer(\n    accelerator='gpu', \n    devices=1\n)\n\n# Test the model\nprint(\"\\nTesting the best model...\")\ntest_results = test_trainer.test(test_runner, test_loader, ckpt_path=best_ckpt)\n\nprint(\"\\nTest Results:\")\nprint(test_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}